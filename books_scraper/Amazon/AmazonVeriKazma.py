import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from tabulate import tabulate

# -------------------------------
# Ayarlar
# -------------------------------
BASE_URL = "https://www.amazon.com.tr/gp/bestsellers/computers/12601907031?pg="
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:118.0) Gecko/20100101 Firefox/118.0",
    "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive"
}
DELAY_RANGE = (2, 5)
MAX_PAGES = 2  # Her sayfada 50 Ã¼rÃ¼n â†’ toplam 100 Ã¼rÃ¼n

# -------------------------------
# YardÄ±mcÄ± Fonksiyonlar
# -------------------------------
def get_soup(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            print(f"   Hata {response.status_code} ile {url}")
            return None
        return BeautifulSoup(response.text, "html.parser")
    except Exception as e:
        print(f"   Ä°stek sÄ±rasÄ±nda hata: {e}")
        return None

def extract_product_links(soup):
    """Ana sayfadan Ã¼rÃ¼n linklerini Ã§ek"""
    links = []
    for li in soup.select("li.zg-no-numbers"):
        a = li.select_one("a.a-link-normal[href*='/dp/']")
        if a:
            href = a["href"].split("?")[0]
            full_link = "https://www.amazon.com.tr" + href
            links.append(full_link)
    return list(dict.fromkeys(links))  # tekrarlarÄ± kaldÄ±r

def extract_product_data(soup, url):
    """ÃœrÃ¼n detay sayfasÄ±ndan bilgileri al"""
    data = {}

    # Ä°sim
    data["isim"] = soup.select_one("#productTitle").get_text(strip=True) if soup.select_one("#productTitle") else "NonePublished"

    # Fiyat
    price_sel = "#priceblock_ourprice, #priceblock_dealprice, span.a-price span.a-offscreen"
    data["fiyat"] = soup.select_one(price_sel).get_text(strip=True) if soup.select_one(price_sel) else "NonePublished"

    # DeÄŸerlendirilme sayÄ±sÄ±
    rating_sel = "#acrCustomerReviewText"
    if soup.select_one(rating_sel):
        try:
            data["deÄŸerlendirilme sayÄ±sÄ±"] = int(soup.select_one(rating_sel).get_text(strip=True).split()[0].replace(".", ""))
        except:
            data["deÄŸerlendirilme sayÄ±sÄ±"] = "NonePublished"
    else:
        data["deÄŸerlendirilme sayÄ±sÄ±"] = "NonePublished"

    # Teknik detaylar
    data["MarkasÄ±"] = data["Modeli"] = data["Ekran boyutu"] = data["iÅŸletim sistemi"] = data["rengi"] = "NonePublished"
    try:
        tables = soup.select("table#productDetails_techSpec_section_1, table#productDetails_detailBullets_sections1")
        for table in tables:
            for row in table.select("tr"):
                th = row.select_one("th")
                td = row.select_one("td")
                if not th or not td:
                    continue
                key = th.get_text(strip=True).lower()
                value = td.get_text(strip=True)
                if "marka" in key:
                    data["MarkasÄ±"] = value
                elif "model" in key:
                    data["Modeli"] = value
                elif "ekran" in key or "display" in key:
                    data["Ekran boyutu"] = value
                elif "iÅŸletim" in key or "operating system" in key:
                    data["iÅŸletim sistemi"] = value
                elif "renk" in key or "colour" in key or "color" in key:
                    data["rengi"] = value
    except:
        pass

    # Link ve gÃ¶rsel
    data["link"] = url
    img_tag = soup.select_one("#landingImage, img#imgBlkFront, img.a-dynamic-image")
    data["img"] = img_tag["src"] if img_tag else "NonePublished"

    return data

# -------------------------------
# Ana Ä°ÅŸ AkÄ±ÅŸÄ±
# -------------------------------
all_products = []
errors = []

for page in range(1, MAX_PAGES + 1):
    print(f"\nðŸ”Ž Sayfa {page} Ã§ekiliyor...")
    soup = get_soup(BASE_URL + str(page))
    if not soup:
        errors.append(f"Sayfa {page} Ã§ekilemedi")
        continue

    links = extract_product_links(soup)
    print(f"    {len(links)} Ã¼rÃ¼n linki bulundu")

    for idx, link in enumerate(links, 1):
        product_soup = get_soup(link)
        if not product_soup:
            errors.append(f"ÃœrÃ¼n Ã§ekilemedi: {link}")
            continue
        product_data = extract_product_data(product_soup, link)
        all_products.append(product_data)

        sleep_time = random.uniform(*DELAY_RANGE)
        print(f"       ÃœrÃ¼n {idx}/{len(links)} Ã§ekildi â†’ {sleep_time:.2f}s bekleniyor...")
        time.sleep(sleep_time)

# -------------------------------
# CSV KaydÄ±
# -------------------------------
df = pd.DataFrame(all_products)
csv_filename = "amazon_tablets_full.csv"
df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
print(f"\nCSV dosyasÄ± kaydedildi: {csv_filename}")

# -------------------------------
# Terminalde tablo olarak gÃ¶ster
# -------------------------------
print("\nÃœrÃ¼n listesi:")
print(tabulate(df, headers="keys", tablefmt="fancy_grid", showindex=True))

# -------------------------------
# Hatalar
# -------------------------------
if errors:
    print("\nHatalar tespit edildi:")
    for e in errors:
        print(" -", e)
else:
    print("\nTÃ¼m Ã¼rÃ¼nler baÅŸarÄ±yla Ã§ekildi")
